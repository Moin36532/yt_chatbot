# -*- coding: utf-8 -*-
"""YTvideo_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUlDWVZDP8qoIBT41kgH5qbox45g58Sm
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install --upgrade --quiet langchain_huggingface
# %pip install --upgrade --quiet langchain_community
# %pip install --upgrade --quiet langchain_core
# %pip install chromadb -q
# %pip install youtube-transcript-api -q
# %pip install fastapi uvicorn pyngrok -q



from dotenv import load_dotenv
from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint
import re
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.proxies import WebshareProxyConfig
from langchain_core.runnables import RunnableLambda
import warnings
warnings.filterwarnings('ignore')


def extract_youtube_id(input_data):
    """
    Extracts the YouTube video ID from a given URL.
    Returns None if no ID is found.
    """
    # If the input is a dictionary, assume the URL is under the key 'URL'
    if isinstance(input_data, dict) and 'URL' in input_data:
        URL = input_data['URL']
    elif isinstance(input_data, str):
        URL = input_data
    else:
        return None # Handle unexpected input types

    # Define the regex pattern to match YouTube IDs
    pattern = r'(?:youtube\.com\/(?:watch\?v=|embed\/)|youtu\.be\/)([^&?\/\s]+)'

    # Search for the pattern in the URL
    match = re.search(pattern, URL)

    # If a match is found, return the video ID (the first group)
    if match:
        return match.group(1)
    else:
        return None
Yt_video_id = RunnableLambda(extract_youtube_id)

from youtube_transcript_api import YouTubeTranscriptApi, WebshareProxyConfig

proxies = [
    ("142.111.48.253", 7030),
    ("31.59.20.176", 6754),
    ("23.95.150.145", 6114),
    ("198.23.239.134", 6540),
    ("45.38.107.97", 6014),
    ("107.172.163.27", 6543),
]

USERNAME = "your_username_here"
PASSWORD = "your_password_here"

def fetch_transcript_with_all_proxies(video_id):
    for ip, port in proxies:
        print(f"Trying proxy {ip}:{port} ...")
        try:
            config = WebshareProxyConfig(
                proxy_host=ip,
                proxy_port=port,
                proxy_username=USERNAME,
                proxy_password=PASSWORD
            )

            yt = YouTubeTranscriptApi(proxy_config=config)
            transcript = yt.fetch(video_id)

            print("SUCCESS with:", ip, port)
            return " ".join([t["text"] for t in transcript])

        except Exception as e:
            print(f"Failed on {ip}:{port} => {e}")

    raise Exception("All proxies failed")

# Example
# result = fetch_transcript_with_all_proxies("3hxE7Af98AI")
# print(result)


    sent = [str(i["text"]) for i in transcript]
    transcript_full = " ".join(sent)
    return transcript_full

transcripter = RunnableLambda(yt_transcript)
# transcript = transcripter.invoke(video_id)
# print(transcript)



def text_splitter(transcript):
  text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=20,
)
  chunks = text_splitter.split_text(transcript)
  return chunks
splitter = RunnableLambda(text_splitter)
# chunks = splitter.invoke(result)
# print(chunks)

def vector_store(chunks):
  embeddings = HuggingFaceEmbeddings(model_name = 'Qwen/Qwen3-Embedding-0.6B')
  store = Chroma(
    collection_name="transcript",
    persist_directory="./transcript",
    embedding_function= embeddings,
)

  store.add_texts(chunks)
  store.persist()
  retriver = store.as_retriever(search_type = 'mmr', search_kwargs = {'k':4})
  docs = (retriver.invoke(question))
  context = ["/n ".join(i.page_content for i in docs)]
  return context
vector = RunnableLambda(vector_store)

def prompt_template(inputs):
    retrive_doc = inputs["context"]
    question = inputs["question"]

    prompt = PromptTemplate(
        template="""Hi! You are a professional chatbot assistant.
        Your duty is to help users answer questions according to the given context.
        If the context is insufficient, just say: "Sorry, I don't know."

        Query:
        {question}

        Context:
        {context}
        """,
        input_variables=["question", "context"]
    )

    prompt1 = prompt.invoke({"question": question, "context": retrive_doc})
    return prompt1

prompt_temp = RunnableLambda(prompt_template)

import os
load_dotenv()
llm = HuggingFaceEndpoint(
    repo_id='deepseek-ai/DeepSeek-V3.1',
    task='text-generation'
)
model = ChatHuggingFace(llm=llm)
# model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

extract_url = RunnableLambda(lambda x: x["URL"])

# 2️⃣ Extract just the question for chain2
extract_question = RunnableLambda(lambda x: x["question"])

from langchain_core.runnables  import RunnableParallel,RunnablePassthrough
chain1  =  extract_url | Yt_video_id  |  transcripter  |  splitter  |  vector
chain2 =  extract_question | RunnablePassthrough()
chain = RunnableParallel({
    'context': chain1,
    'question' : chain2
    })
chain3 = prompt_temp | model
chain_final = chain | chain3








